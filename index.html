<html>
<head>
	<style>
		body{
			color: #333;
		}
		#container{
			min-width: 1000px;
			width: 1000px;
/*			overflow: auto;
*/			margin: 50px auto;padding: 30px;
			/*zoom: 1;*/
*//*			border: 1px solid #ccc;background: #fc9;color: #fff;
*/		}
		#left{
			float: left;
			width: 300px;
			height: 200px;
			margin-left: 0px;
		}
		#right{
			float: left;
			width: auto;
			margin-left: 50px;
		}
		#name{
			font-size: 22.0pt;
		    mso-bidi-font-size: 24.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		        font-weight: bold;
		}
		#info{
		    font-size: 16.0pt;
		    mso-bidi-font-size: 17.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		    margin-top: 30px;
		    margin-left: 5px;
		    margin-bottom: 10px;
		    
		}
		.clear{clear:both; height: 0; line-height: 0; font-size: 0}
		.Bio{
			font-size:16.0pt;
			mso-bidi-font-size:17.0pt;
			line-height:150%;
			font-family:Times;
			mso-bidi-font-family:Lato-Regular;
			text-align: justify;
		}
		span.SpellE {
		    mso-style-name: "";
		    mso-spl-e: yes;
		}
		span.Title{
			    font-size: 22.0pt;
			    mso-bidi-font-size: 17.0pt;
			    font-family: Times;
			    mso-bidi-font-family: Lato-Regular;
			    font: bold;
		}
		div.section{
			padding-top: 30px;
		}
		
		div.sub-left{
			float: left;
			width: 250px;
						
		}
		div.sub-left img{
			vertical-align: middle;
			horizontal-align: middle;
			margin-top: 10px;
		}
		
		div.sub-left span{
			height: 100%;
			display: inline-block;
			vertical-align: top;
						
		}
		div.sub-right{
			float: left;
			width: 650px;			
		}
		.paper{
			overflow: auto;
			zoom:1;
			border-top: 2px dashed #333;
			padding-bottom: 0px;
			min-height: 150px;

		}
		.paperTitle{
			font-size:14.0pt;
			mso-bidi-font-size:18.0pt;
			font-family:Times;
			mso-bidi-font-family:Times;
			margin-top: 10px;
			margin-bottom: 10px;
			font-weight: bold;
		}
		.paperName,.paperPub{
		    font-size: 12.0pt;
		    mso-bidi-font-size: 13.0pt;		    
		    font-family: Times;
		    mso-bidi-font-family: Times;
		    line-height:150%;
		}
		.link{
		    font-size: 12.0pt;
		    mso-bidi-font-size: 13.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		    margin-top: 10px;
		    margin-bottom: 0px;
		}
		.special{
		    margin-top: 0in;
		    margin-bottom: 0in;
		    margin-left: -.9pt;
		    margin-bottom: .0001pt;
		    text-indent: .9pt;
		    mso-pagination: none;
		    tab-stops: 13.75in;
		    mso-layout-grid-align: none;
		    text-autospace: none;
		}
		.long div.sub-left, .long div.sub-right{
			height:315px;

		}
		.short div.sub-left, .short div.sub-right{
			height:150px;

		}
		div.sub-left,div.sub-right{
			height:200px;

		}
	</style>
</head>
<body>
	<div id="container">
		<div id="left">
			<img width="300" height="225" src="./imgs/photo.jpg" />
		</div>
		<div id="right">
			<div id="name">Yingqian Wang（王应谦）</div>
			<div id="info">

				Ph.D. Student<p>
				National University of Defense Technology (NUDT), China<p>
				Email: wangyingqian16@nudt.edu.cn<p>				
			</div>

			         <a href="https://scholar.google.com/citations?user=tBA4alMAAAAJ&hl=en" target="_blank" rel="nofollow">Google Scholar</a>  |
			         <a href="https://www.researchgate.net/profile/Yingqian_Wang3?ev=prf_highl" target="_blank" rel="nofollow"><span>Research Gate</span></a>  |
			         <a href="https://github.com/YingqianWang" target="_blank" rel="nofollow"><span>Github</span></a>  |
			         <a href="https://blog.csdn.net/weixin_38490884" target="_blank" rel="nofollow"><span>Blog</span></a>
			</div>

		<div class="clear"></div>
		<div class="section">
			<span class="Title"><b>Brief Bio</b></span><p>			
				<div class="Bio">
				I received my B.E. degree from Shandong University in 2016, and received my M.E. degree from NUDT in 2018.
					Currently, I'm working toward the Ph.D. degree with the College of Electronic Science and Technology, NUDT.
					My research interests mainly focus on low-level vision, particularly on
					<b style="mso-bidi-font-weight:normal">light field imaging</b> and <b style="mso-bidi-font-weight:normal">image super-resolution</b>.</span></p>
				</div>


	<div class="section">
		<span class="Title"><b>News</b></span><p>			
		<div class="paperPub"><b>
			2020.06 | We release the source code, pretrained models, and test datasets of LF-InterNet. <br>
			2020.04 | Two papers on Scale-Arbitrary Super-Resolution and Video Super-Resolution are posted on arXiv. <br>			
			2020.02 | Our paper "A Stereo Attention Module for Stereo Image Super-Resolution" is accepted by IEEE SPL. <br>
			2019.12 | Our paper "Spatial-Angular Interaction for Light Field Image Super-Resolution" is posted on arXiv. <br>
			2019.12 | Our paper "DeOccNet: Learning to See Through Foreground Occlusions in Light Fields" is accepted by WACV 2020. <br>
			2019.03 | A large-scale dataset for stereo image super-resolution is now available <a href="https://yingqianwang.github.io/Flickr1024" target="_blank" rel="nofollow">here</a>.<br>
			2019.02 | Our paper "Learning Parallax Attention for Stereo Image Super-Resolution" is accepted to <span style="color:red">CVPR 2019</span>! <br>

		</b></div>
	</div>
	
	
	<div class="clear"></div>
	<div class="section">
	<span class="Title"><b>Publications --- 2020</b></span><p><p>


                        <div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/LF-InterNet.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Spatial-Angular Interaction for Light Field Image Super-Resolution
					</div>
					<div class="paperName">
						<b>Yingqian Wang</b>, Longguang Wang, Jungang Yang, Wei An, Jingyi Yu, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"><b>New!</b> </span> arXiv, 2020.<br> 
						<span style="color:blue">We have compared our LF-InterNet with LF-ATO (CVPR2020) in the current version.</span><br>
						| <a href="https://arxiv.org/pdf/1912.07849.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://github.com/YingqianWang/LF-InterNet" target="_blank" rel="nofollow">Code</a>
					</div>
				</div>
			</div>

			
			<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/ArbSR.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Learning for Scale-Arbitrary Super-Resolution from Scale-Specific Networks
					</div>
					<div class="paperName">
						Longguang Wang, <b>Yingqian Wang</b>, Zaiping Lin, Jungang Yang, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"><b>New!</b> </span> arXiv, 2020.<br> 
						<span style="color:blue">A plug-in module to extend SISR networks for scale-arbitrary (non-integer and asymmetric) SR.</span><br>
						| <a href="https://arxiv.org/pdf/2004.03791.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://github.com/LongguangWang/Scale-Arbitrary-SR" target="_blank" rel="nofollow">Code</a>
					</div>
				</div>
			</div>
			
			<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/D3Dnet.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Deformable 3D Convolution for Video Super-Resolution
					</div>
					<div class="paperName">
						Xinyi Ying, Longguang Wang, <b>Yingqian Wang</b>, Weidong Sheng, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"><b>New!</b> </span> arXiv, 2020.<br> 
						<span style="color:blue">Powerful in motion-aware and spatio-temporal modeling.</span><br>
						| <a href="https://arxiv.org/pdf/2004.02803.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://github.com/XinyiYing/D3Dnet" target="_blank" rel="nofollow">Code</a>
					</div>
				</div>
			</div>			
			
			
						

			

			<div class="paper short">
				<div class="sub-left">
					<span></span>					
					<img border="0" width="200" height="130" src="imgs/SAM.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						A Stereo Attention Module for Stereo Image Super-Resolution
					</div>
					<div class="paperName">
						Xinyi Ying*, <b>Yingqian Wang*</b>, Longguang Wang, Weidong Sheng, Wei An, Yulan Guo 
					</div>
					<div class="paperPub">
						IEEE Signal Processing Letters, vol. 27, pages. 496-500, 2020. (* co-first authors) <br>
						<span style="color:blue">A generic module to extend arbitrary SISR networks for stereo image SR.</span><br>					
						| <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8998204" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://github.com/XinyiYing/SAM" target="_blank" rel="nofollow">Code</a>
						| <a href="https://mp.weixin.qq.com/s/TyCsUMyoya86wJRiCIXtKA" target="_blank" rel="nofollow">Report (Chinese)</a>						
					</div>
				</div>
			</div>
			
			<div class="paper short">
				<div class="sub-left">
					<span></span>					
					<img border="0" width="200" height="130" src="imgs/DeOccNet.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						DeOccNet: Learning to See Through Foreground Occlusions in Light Fields
					</div>
					<div class="paperName">
						<b>Yingqian Wang</b>, Tianhao Wu, Jungang Yang, Longguang Wang, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						IEEE Winter Conference on Applications of Computer Vision (WACV), 2020.<br>
						<span style="color:blue">The first deep learning approach for light field de-occlusion.</span><br>	
						| <a href="https://arxiv.org/pdf/1912.04459.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://github.com/YingqianWang/DeOccNet" target="_blank" rel="nofollow">Code&Dataset</a>
						| <a href="https://mp.weixin.qq.com/s/0K_NF84wvPJttEARVUGPWA" target="_blank" rel="nofollow">Report (Chinese)</a>	
						| <a href="https://yingqianwang.github.io/DeOccNet/Poster.pdf" target="_blank" rel="nofollow">Poster</a>
						| <a href="https://youtu.be/vqHprbEFFis" target="_blank" rel="nofollow">Video Presentation</a>
					</div>
				</div>
			</div>			


	<div class="clear"></div>
	<div class="section">
	<span class="Title"><b>Publications --- 2019</b></span><p><p>

			<div class="paper short">
				<div class="sub-left">
					<span></span>					
					<img border="0" width="200" height="130" src="imgs/Flickr1024.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Flickr1024: A Large-Scale Dataset for Stereo Image Super-Resolution
					</div>
					<div class="paperName">
						<b>Yingqian Wang</b>, Longguang Wang, Jungang Yang, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						ICCV Workshops, 2019.<br>
						<span style="color:blue">Contains 1024 high-quality stereo images and covers diverse scenarios.</span><br>	
						| <a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/LCI/Wang_Flickr1024_A_Large-Scale_Dataset_for_Stereo_Image_Super-Resolution_ICCVW_2019_paper.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://yingqianwang.github.io/Flickr1024" target="_blank" rel="nofollow">Dataset</a>					  
					</div>
				</div>
			</div>

			<div class="paper short">
				<div class="sub-left">
					<span></span>					
					<img border="0" width="200" height="120" src="imgs/PASSRnet.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Learning Parallax Attention for Stereo Image Super-Resolution
					</div>
					<div class="paperName">
						Longguang Wang, <b>Yingqian Wang</b>, Zhengfa Liang, Zaiping Lin, Jungang Yang, Wei An, Yulan Guo
					</div>
					<div class="paperPub">
						IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2019.<br>
						<span style="color:blue">Extend attention mechanism to stereo images for super-resolution.</span><br>							
						| <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Learning_Parallax_Attention_for_Stereo_Image_Super-Resolution_CVPR_2019_paper.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://github.com/LongguangWang/PASSRnet" target="_blank" rel="nofollow">Code</a>
						| <a href="https://yingqianwang.github.io/Flickr1024" target="_blank" rel="nofollow">Dataset</a>
						| <a href="https://mp.weixin.qq.com/s/zN11cI3dOOp1PDXaCPcRng" target="_blank" rel="nofollow">Report (Chinese)</a>
					</div>
				</div>
			</div>


			<div class="paper short">
				<div class="sub-left">
					<span></span>					
					<img border="0" width="200" height="130" src="imgs/SPL2019.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Selective Light Field Refocusing for Camera Arrays Using Bokeh Rendering and Superresolution
					</div>
					<div class="paperName">
						<b>Yingqian Wang</b>, Jungang Yang, Yulan Guo, Chao Xiao, Wei An
					</div>
					<div class="paperPub">
						IEEE Signal Processing Letters, Vol. 26, Issue. 1, 2019. <br>						
						| <a href="https://yingqianwang.github.io/Selective-LF-Refocusing/wang2018selective.pdf" target="_blank" rel="nofollow">Paper</a>
						| <a href="https://github.com/YingqianWang/Selective-LF-Refocusing" target="_blank" rel="nofollow">Code</a>					  
					</div>
				</div>
			</div>
			
		

			<div class="section">
				<span class="Title"><b>Teaching Assistance</b></span><p>			
				<div class="paperPub"><b>
					Lecture: Signals and Systems (Spring Term, 2020)<br>
					Lecture: Optical Detection (Autumn Term, 2019)<br>
					Lecture: Optical Detection (Autumn Term, 2018)<br>
				</b></div>
			</div>
			

			<div class="section">
				<span class="Title"><b>Awards & Honors</b></span><p>			
				<div class="paperPub"><b>
					2018 | Guanghua Scholarship<br>
					2016 | Excellent Graduates of Shandong Province<br>
					2015 | National Scholarship (Ministry of Education, Top 2%)<br>
					2015 | The 1st Prize in the Final of China Mathematics Competitions (45 winners over 63K participants, Top 0.07%)<br>
					2015 | The 1st Prize in China Mathematics Competitions<br>					
					2014 | National Scholarship (Ministry of Education, Top 2%)<br>
					2014 | The 1st Prize in China Mathematics Competitions<br>
					2013 | National Scholarship (Ministry of Education, Top 2%)<br>
					2013 | The 1st Prize in China Mathematics Competitions<br>
				</b></div>
			</div>

	</div>
	
</body>
</html>
